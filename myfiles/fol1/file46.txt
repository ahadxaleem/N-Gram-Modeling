ct of the Center for Open Science, to assess the extent of the problem found that as many as two-thirds of highly publicized findings in psychology failed to be replicated. Reproducibility has generally been stronger in cognitive psychology (in studies and journals) than social psychology and subfields of differential psychology. Other subfields of psychology have also been implicated in the replication crisis, including clinical psychology, developmental psychology, and a field closely related to psychology, educational research.

Focus on the replication crisis has led to other renewed efforts in the discipline to re-test important findings. In response to concerns about publication bias and data dredging (conducting a large number of statistical tests on a great many variables but restricting reporting to the results that were statistically significant), 295 psychology and medical journals have adopted result-blind peer review where studies are accepted not on the basis of their findings and after the studies are completed, but before the studies are conducted and upon the basis of the methodological rigor of their experimental designs and the theoretical justifications for their proposed statistical analysis before data collection or analysis is conducted. In addition, large-scale collaborations among researchers working in multiple labs in different countries have taken place. The collaborators regularly make their data openly available for different researchers to assess. Allen et al. estimated that 61 percent of result-blind studies have yielded null results, in contrast to an estimated 5 to 20 percent in traditional research.

Misuse of statistics
Further information: Misuse of statistics and Misuse of p-values
Some critics view statistical hypothesis testing as misplaced. Psychologist and statistician Jacob Cohen wrote in 1994 that psychologists routinely confuse statistical significance with practical importance, enthusiastically reporting grea